# -*- coding: utf-8 -*-
"""tinyllama-finetune

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rKlcsjWp-kZdiVl12nWcXFxO3Zd0pWvt
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers datasets peft accelerate trl sentencepiece

!pip install faiss-cpu sentence-transformers

from datasets import load_dataset

# Load the Alpaca-cleaned dataset
dataset = load_dataset("yahma/alpaca-cleaned")
print(dataset["train"][0])  # Check the first example in the training set

!apt-get install git-lfs

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

print("Model and tokenizer loaded successfully!")

from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from transformers import Trainer, DataCollatorForLanguageModeling
from datasets import load_dataset

# Load model and tokenizer
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
dataset = load_dataset("yahma/alpaca-cleaned")["train"]
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# Tokenize the dataset
def tokenize(example):
    return tokenizer(f"### Instruction:\n{example['instruction']}\n### Input:\n{example['input']}\n### Response:\n{example['output']}", padding="max_length", truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize)

# Load the model
model = AutoModelForCausalLM.from_pretrained(model_name)

# Set up LoRA configuration
config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM,
)
model = get_peft_model(model, config)

# Training arguments (disable W&B logging)
training_args = TrainingArguments(
    output_dir="./results",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    max_steps=500,  # Limit total steps to 500 for quick testing
    learning_rate=2e-4,
    fp16=True,
    logging_steps=20,
    save_steps=250,
    save_total_limit=2,
    logging_dir="./logs",
    report_to="none",  # Disable W&B for now to simplify
)


# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

# Fine-tune the model
trainer.train()

!pip install faiss-cpu

from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import torch

# Load the tokenizer and model
tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
tokenizer.pad_token = tokenizer.eos_token  # Ensure pad_token is set

model = AutoModelForCausalLM.from_pretrained(
    "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
)
model.eval()

# Load sentence transformer for embeddings
embed_model = SentenceTransformer('all-MiniLM-L6-v2')

# Replace with your knowledge base
documents = [
    "LoRA enables efficient fine-tuning by updating only a few parameters.",
    "TinyLLaMA is a lightweight causal language model suited for low-resource environments.",
    "Quantum computing is based on quantum bits or qubits that can exist in multiple states."
]

# Generate embeddings
doc_embeddings = embed_model.encode(documents).astype("float32")

# Create FAISS index
dimension = doc_embeddings.shape[1]
faiss_index = faiss.IndexFlatL2(dimension)
faiss_index.add(doc_embeddings)

def rag_generate(query, top_k=2):
    # Embed the query
    query_embedding = embed_model.encode([query]).astype("float32")
    _, indices = faiss_index.search(query_embedding, top_k)

    # Retrieve top-k documents
    context = "\n".join([documents[i] for i in indices[0]])

    # Construct RAG-style prompt
    prompt = f"""### Instruction:
Use the following context to answer the question.

### Context:
{context}

### Question:
{query}

### Response:
"""

    # Tokenize and move to GPU if needed
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    if torch.cuda.is_available():
        inputs = {k: v.to("cuda") for k, v in inputs.items()}
        model.to("cuda")

    # Generate response
    outputs = model.generate(
        **inputs,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
        top_p=0.9
    )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Test the function
response = rag_generate("Explain the concept of quantum computing in simple terms.")
print(response)